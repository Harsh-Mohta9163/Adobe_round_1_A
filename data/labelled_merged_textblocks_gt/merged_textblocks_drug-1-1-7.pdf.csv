text,bbox,page_number,avg_font_size,word_count,is_all_caps,char_density,ratio_of_verbs,ratio_capitalized,ends_with_colon,is_bold,normalized_vertical_gap,indentation_change,same_alignment,is_centered_A,font_size_diff,same_font,is_bold_A,is_italic_A,is_monospace_A,same_bold,same_italic,same_monospace,line_a_ends_punctuation,line_b_starts_lowercase,is_linea_in_rectangle,is_lineb_in_rectangle,both_in_table,neither_in_table,is_linea_hashed,is_lineb_hashed,both_hashed,neither_hashed,title_label
OPEN www.nature.com/scientificreportshttp://www.nature.com/scientificreports,"[0, 0, 100, 20]",1,14,2,0,0.0375,0,0.1111111111111111,0,0,-20,0,1,0,8,0,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
Drug discovery and mechanism prediction with explainable graph neural networks,"[0, 0, 100, 20]",1,26,10,0,0.0345,0,0.1,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,1
"Conghao Wang, Gaurav Asok Kumar & Jagath C. Rajapakse ?","[0, 0, 100, 20]",1,10,10,0,0.023,0,1,0,0,0,0,1,1,0,1,0,0,0,1,1,1,1,0,0,0,0,1,0,0,0,1,0
"Apprehension of drug action mechanism is paramount for drug response prediction and precision medicine. The unprecedented development of machine learning and deep learning algorithms has expedited the drug response prediction research. However, existing methods mainly focus on forward encoding of drugs, which is to obtain an accurate prediction of the response levels, but omitted to decipher the reaction mechanism between drug molecules and genes. We propose the eXplainable Graph-based Drug response Prediction XGDP approach that achieves a precise drug response prediction and reveals the comprehensive mechanism of action between drugs and their targets. XGDP represents drugs with molecular graphs, which naturally preserve the structural information of molecules and a Graph Neural Network module is applied to learn the latent features of molecules. Gene expression data from cancer cell lines are incorporated and processed by a Convolutional Neural Network module. A couple of deep learning attribution algorithms are leveraged to interpret interactions between drug molecular features and genes. We demonstrate that XGDP not only enhances the prediction accuracy compared to pioneering works but is also capable of capturing the salient functional groups of drugs and interactions with significant genes of cancer cells.","[0, 0, 100, 20]",1,9,191,0,0.5715,0.15706806282722513,0.09895833333333333,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"Aiming at facilitating precision medicine in complex disease such as cancer, computational approaches have been increasingly proposed to delve into the reactions between drugs and cancer cells 1 . Recently, numerous machine learning 2,3 and deep learning 4,5 methods have been successfully applied to predict drug response levels precisely. However, most of them target at phenotypic screening 6 and do not come along with a reasonable interpretability, rendering drug reaction mechanism obscure. To expedite precision medicine, it is crucial to elucidate the mechanism of action of drugs and thereby promote novel drug discovery.","[0, 0, 100, 20]",1,9,93,0,0.269,0.21739130434782608,0.0425531914893617,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"A proper representation of a drug molecule is pivotal to any drug response prediction methods. According to recent reviews of molecular representations of drugs 7, there are mainly three categories of representation: linear notations, molecular fingerprints FPs, and graph notations. Linear notations encode the molecule with a vector of string. Two frequently used instances of linear notations are the IUPAC International Chemical Identifier InChI 8, and the Simplified Molecular-Input Line-Entry System SMILES 9 . SMILES strings are more widely used since it encodes the chemical structure into a string of ASCII characters. CaDRReS 10 applied Matrix Factorization to learn the latent features of drugs with the cell line gene expression data and drug sensitivity matrix, and compared the similarity scores derived from learned features and SMILES notations. tCNNs 11 and CDRScan 12 adopted Convolutional Neural Networks CNN to learn a latent representation of drugs? SMILES vector. CNN is a powerful deep learning approach to handle grid-like data in the domain of texts and images, which can be used to encode the linear notations of drugs as well. However, the SMILES notation does not possess the property of locality like texts and images since the physically adjacent atoms in the sequence of SMILES string can be far away from each other in the real molecular environment, and therefore dimisses the structural information of molecules.","[0, 0, 100, 20]",1,9,223,0,0.612,0.11711711711711711,0.14666666666666667,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"Molecular fingerprints, such as Molecular Access System MACCS 13 and Chemically Advanced Template Search 14, identify the key structures of a molecule and represent them with a binary vector where each bit denotes the structure?s existence. A drawback of this kind of representation is that only the pre-defined structure can be recognized, which might hamper the discovery of novel structures. To circumvent this problem, circular fingerprints such as Extended Connectivity FPs ECFPs based on Morgan algorithm 15 has been proposed to iteratively search the substructures of molecules rather than pre-define them. The information of these crucial structures is preserved in this kind of representation, whereas the positional information is lost, and we can hardly track where these sub-structures occur in the molecule. DeepDSC 16 combines Morgan fingerprints of drugs into the latent features of cancer cell lines learned by an auto-encoder. S2DV 17 applied word2vec 18 to","[0, 0, 100, 20]",1,9,147,0,0.414,0.14965986394557823,0.13157894736842105,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,0
"College of Computing and Data Science, Nanyang Technological University, Singapore 639798, Singapore. ? email: ASJagath@ntu.edu.sg","[0, 0, 100, 20]",1,8.5,15,0,0.058,0,0.5882352941176471,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,1,0,0,0,0,1,0,0,0,1,0
Scientific Reports 2025 15:179 https://doi.org/10.1038/s41598-024-83090-3 1,"[0, 0, 100, 20]",1,8,6,0,0.035,0,0.13333333333333333,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,0
"tokenize ECFP features or SIMLES as drugs? representations. Ma et al. used Atom Pairs AP, MACCS and circular fingerprints as the descriptor of drugs, and performed the quantitative structure activity relationship QSAR study with a Deep Neural Network 19 . Graph notations have recently been brought under the spotlight in the domain of drug representation. Previously, compromising on computational complexity of molecular structures and the confined power of graph learning, aforementioned methods are preferred to denote a molecule even at the cost of loss of information. However, with the advent of Graph Neural Networks GNN in the deep learning domain in recent years, it is now feasible to store and analyze the information from molecules in graphs 20 .","[0, 0, 100, 20]",2,9,119,0,0.3205,0.11965811965811966,0.15384615384615385,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"Numerous variants of GNN models have been applied in the pharmaceutical domain 21,22 and demonstrated to learn the latent representation of the molecular graphs trading off the descriptive power against complexity. A Graph Convolution Network GCN model 23 was proposed to predict the chemical properties of molecules and discover porous materials. The typical message passing pattern of GNN intrinsically weakens the influence of distal nodes, which might contradict the real case in the molecule, where atoms from a long topological distance can still interact such as intramolecular hydrogen bonds. An Attentive FP model proposed by 24 leveraged the graph attention mechanism to learn the impact of a node to another. This model addressed the above issue by updating the nodes with a trade-off between the topological distance and the possibly intangible linkage with the attention mechanism. GraphDRP 25 enhanced the tCNN 11 prediction precision by substituting the drugCNN module with GNN to better encapsulate the drug features. DeepCDR 26, TGSA 27 and DualGCN 28 further explored integrating multi-omics profiles for a better representation of cancer cell lines. Besides modeling drugs with GNN, SWNet 29 introduced a self-attention mechanism to bring drug similarity into the consideration when learning cell features. An algebraic graph-assisted bidirectional transformer AGBT model 30 was developed to encode the 3D structure of molecules into algebraic graphs. And Molecular Topographic Map MTM was generated from atom features by using Generative Topographic Mapping GTM 31 to represent drugs in graphs 32 . In this study, we propose a framework named eXplainable Graph-based Drug response Prediction XGDP for predicting anti-cancer drug responses and discovering the mechanism of action. The architecture of XGDP, as shown in Fig. 1, is composed of 3 modules. The GNN module learns the latent features of drugs denoted by molecular graphs. We propose to use a set of novel features adapted from ECFPs as the node features and incorporate chemical bond types as the edge features in our graph convolutional layers. And the CNN module learns the latent features of cancer cell lines from its gene expression profiles. Then, a cross-attention module is utilized to integrate latend features from drugs and cell lines, and thereafter predict the drug responses. The experimental results indicate that, with novel node and edge features, our model outperformed the previous drug response prediction methods 11,25 . Moreover, we leverage deep learning attribution approaches such as GNNExplainer 33 and Integrated Gradients 34 to interpret our model. It is demonstrated that our developed model is capable of identifying the active substructures of drugs and the significant genes in cancer cells, and thus revealing the mechanism of action of drugs.","[0, 0, 100, 20]",2,9,436,0,1.2105,0.14055299539170507,0.11963882618510158,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
Methods,"[0, 0, 100, 20]",2,11,1,0,0.0035,0,1,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,1
Datasets,"[0, 0, 100, 20]",2,10,1,0,0.004,0,1,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,1
We propose a deep learning-based approach to predict the drug responses of cancer with molecular graphs of drugs and gene expression data from cancer cell lines. The dataset was acquired from Genomics of Drug,"[0, 0, 100, 20]",2,9,34,0,0.0875,0.11764705882352941,0.11428571428571428,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"Fig. 1 . The architecture of the proposed model XGDP for drug response and mechanism prediction. Molecular graph, node features and edge features are extracted from the drug molecule, and GNN is used for learning the latent features of drugs. CNN is applied to compress the gene expression features from cancer cell lines. Then two multi-head cross-attention layers are leveraged to combine drug and cell features, and the drug response is predicted with the integrated features. Scientific Reports 2025 15:179 https://doi.org/10.1038/s41598-024-83090-3 2","[0, 0, 100, 20]",2,8.83,82,0,0.237,0.17073170731707318,0.09782608695652174,0,0,-20,0,1,0,1,0,0,0,0,1,1,1,1,0,0,0,0,1,0,0,0,1,0
"Sensitivity in Cancer GDSC database 35, including response levels in IC50 formats, drug names, and cell line names. Gene expression data of cell lines are obtained from Cancer Cell Line Encyclopedia CCLE 36 . Drugs? names are retrieved in PubChem database 37 to obtain their SMILES vectors. Then the SMILES vectors are converted into molecular graphs with RDKit library 38 .","[0, 0, 100, 20]",3,9,61,0,0.157,0.13559322033898305,0.2711864406779661,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"We combine the GDSC and CCLE datasets by selecting cell lines whose drug responses and gene expression profiles are both recorded. In total, there are 223 drugs and 700 cell lines. After removing missing screening of drug responses, 133,212 pairs of data points are left for experiments. Each cell line is depicted by a transcriptomic profile of 13,142 genes. In order to reduce the dimensionality of the input features to avoid potential over-fitting in model training, we refer to the connectivity map proposed in LINCS L1000 research 39, and preserve only the expression values of the 956 landmark genes, since it is testified that the expression pattern of other genes can be precisely inferred by the landmark genes.","[0, 0, 100, 20]",3,9,118,0,0.302,0.17796610169491525,0.0743801652892562,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
Drug representation,"[0, 0, 100, 20]",3,10,2,0,0.009,0,0.5,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,1
"Previous research have demonstrated that representing drugs with molecular graphs provides better predictive power than compressed representations such as SMILES 11,25, since the structural information of a molecule can be naturally preserved in a graph. Specifically, by considering the atoms in a molecule as nodes and the chemical bonds between atoms as edges, an undirected unweighted graph is constructed to represent the drug molecule. From the molecular graphs, node features proposed by DeepChem 40 such as atom symbol, atom degree, etc., can be extracted.","[0, 0, 100, 20]",3,9,84,0,0.2405,0.15476190476190477,0.058823529411764705,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"In this chapter, we further enhance the predictive power of a drug?s graph representation by incorporating properer node and edge features. In the previous work 25, there are five types of node features, i.e., atom symbol, atom degree, the total number of Hydrogen, implicit value of atom, and whether the atom is aromatic. Nevertheless, these features are intuitively restricted to depict an atom in a molecule. Inspired by the Morgan Algorithm and Extended-Connectivity Fingerprints ECFP 15, we present a circular algorithm to compute the feature of an atom, considering both the atom itself and its surrounding environment.","[0, 0, 100, 20]",3,9,97,0,0.265,0.12371134020618557,0.11,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
Algorithm 1 . Circular atomic feature computation,"[0, 0, 100, 20]",3,9,7,0,0.0215,0,0.3333333333333333,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,0
"In Circular Atomic Feature Computation Algorithm 1, F i refers to the chemical properties of atom i to be encoded, which involves the seven Daylight atomic invariants as the initial chemical properties, including number of immediate neighbors who are non-hydrogen atoms, the valence minus the number of hydrogens meaning total bond order ignoring bonds to hydrogens, the atomic number, the atomic mass, the atomic charge, the number of attached hydrogens, and aromaticity. X i r  denotes the identifier of atom  i  after collecting features from its r -hop neighbour atoms. h is the hashing function used for feature compression and binary is the function to convert hashed integers back to binary features. Operator ? refers to the concatenation operation. Scientific Reports 2025 15:179 https://doi.org/10.1038/s41598-024-83090-3 3","[0, 0, 100, 20]",3,8.88,125,0,0.353,0.136,0.08955223880597014,0,0,-20,0,1,0,1,0,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"Figure 2 provides an example of the feature extraction procedure of atom 2 in the Butyramide molecule considering interested radius of 1. In particular, this algorithm involves three stages:","[0, 0, 100, 20]",4,9,29,0,0.081,0.10344827586206896,0.10344827586206896,1,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
1. Initial Stage: Each atom in the molecule is assigned with a unique integer identifier which is generated by hashing a set of chemical properties.,"[0, 0, 100, 20]",4,9,25,0,0.0625,0.2,0.12,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"2. Updating Stage: After initialization, each atom?s identifier will be updated iteratively. Starting by radius","[0, 0, 100, 20]",4,9,15,0,0.049,0.26666666666666666,0.25,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,0
"a An array will be formed by collecting the radius and the core atom?s current identifier. b Next, the neighboring information of the atom that is r hops away from the core atom will be incorpo­ rated into the array. Ranked by the bond order single, double, triple, and aromatic, the bond order and the current identifier of the interested atom are appended to the array. c Then the same hash function used in the initial stage is applied again to convert the array into a new integer identifier. d The above procedure is repeated for each atom in the molecule. e The radius will be updated as r := r + 1 and another iteration to update identifiers for all atoms will be started, unless r has already met the user?s interested radius.","[0, 0, 100, 20]",4,9,134,0,0.303,0.18181818181818182,0.04477611940298507,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,1,1,0,0,0,1,0,0,0,1,0
"3. Reduction Stage: Eventually, for each atom, all the identifiers ever generated in the updating stage are con­ verted into a 64-bit binary vector by calculating the modulus of the decimal integer and concatenated to form the final atom feature vector of length 64 × radius + 1 .In the typical ECFP algorithm, the updating stage is aimed at discovering the unique substructures in the molecule, which will be consequently integrated into the fingerprint serving as a molecular-level representation. Thus, after the updating iterations, there will be a duplicate structure removal stage to eliminate the identical features which encapsulates the same sur­ rounding environment of atoms. However, on the contrary to a molecular-level representation, in our case we require an atom-level feature where the structural duplication amongst atoms is not hampering feature reduction. Besides, the original algorithm only considers the last generated identifiers upon reducing them into the fingerprint, which is effective in producing the unique fingerprint of the molecule, whereas we pre­ serve all the ever-generated identifiers to produce the atom-level features. This is because the last identifier is always computed considering a relatively large radius. The surrounding substructure might thus be identical for different atoms. Therefore, if merely considering the last identifier, certain atom-level features may be duplicated, and the corresponding atoms will be indistinguishable. Under such circumstances, identifiers generated at all radius levels are appended to form the atom-level feature.","[0, 0, 100, 20]",4,9,233,0,0.6835,0.16810344827586207,0.0502092050209205,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"Fig. 2 . Illustration of feature extraction procedure of atom 2 in the Butyramide molecule. In the initial stage, chemical features including number of non-hydrogen neibours, valency, atomic number, etc., are extracted and hashed to compute the initial identifier of each atom. In the update stage, starting from radius of 1, the bond orders and identifiers of the surrounding atoms atom 3, 6 and 1 are combined and concatenated with the iteration number and the initial identifier of the focused atom atom 2. The hash function is applied again on the concatenated feature to form the new identifier. Finally, in the reduction stage, each of the identifiers of atom 2 generated in the update stage are converted to a 64-bit binary vector and concatenated to form the final atom feature. In our implementation, we combined the binary vectors of radius 0, 1, 2 and 3, which forms a the final feature vector of length Scientific Reports 2025 15:179 https://doi.org/10.1038/s41598-024-83090-3 4","[0, 0, 100, 20]",4,8.9,160,0,0.4155,0.13125,0.058823529411764705,0,0,-20,0,1,0,1,0,0,0,0,1,1,1,1,1,0,0,0,1,0,0,0,1,0
Computational framework,"[0, 0, 100, 20]",5,10,2,0,0.011,0,0.5,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,1
"We utilize Graph Neural Networks GNN to learn the latent features of drugs? molecular graphs and Convolutional Neural Networks CNN to learn the representation of the gene expression data, and combine them together to predict the response level as shown in Fig. 1. Instead of concatenating latent features of drug and cell line as tCNN 11 and GraphDRP 25, we propose to leverage multi-head attention mechanism introduced by Transformer 41 to integrate the drug and cell line features effectively.","[0, 0, 100, 20]",5,9,79,0,0.2085,0.1518987341772152,0.1625,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,0
Each head H i in the multi-head attention module can be formulated as,"[0, 0, 100, 20]",5,9,13,0,0.0285,0.15384615384615385,0.14285714285714285,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,0
"where Q, K and V stand for the query, key and value used in an attention layer. To obtain the drug embed influenced by gene expressions, we use drug features encoded by GNN as Q and cell line features encoded by CNN as K and V . On the contrary, to learn the gene embed, we use cell line features as Q and drug features as K and V . Eventually, the integrative features are combined and fed into a predictor composed of a dense layer for drug response prediction.","[0, 0, 100, 20]",5,9,90,0,0.187,0.17045454545454544,0.1590909090909091,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"After developing the model, we adopt Integrated Gradients 34 and GNNExplainer 33 to explore the saliency of inputs, i.e., atoms and bonds of the drug molecule and transcriptomic features of the cell line, which reveals the reaction mechanism of cancer cell lines and drugs.","[0, 0, 100, 20]",5,9,44,0,0.115,0.09090909090909091,0.08888888888888889,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
Graph neural networks GNN,"[0, 0, 100, 20]",5,10,4,0,0.011,0,0.5,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,1
"After constructing the molecular graphs and extracting atom-level features for the drugs, we develop GNN models to further learn the latent representation of the drugs. In this work, we take advantage of four types of GNN models and compare their performance in drug response and mechanism prediction: Graph Convolutional Networks GCN 42, Graph Attention Networks GAT 43, Relational Graph Convolutional Networks RGCN 44, and Relational Graph Attention Networks RGAT 45 . Similarly, the idea for such GNN models is to aggregate the information from a node itself and its neighborhood.","[0, 0, 100, 20]",5,9,90,0,0.247,0.0898876404494382,0.26666666666666666,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"If we define the atom set in a drug?s molecule as V and the bond set as E, the molecular graph of this drug can be given by G =  V, E  . Then we use an adjacent binary matrix A ? R N ×N to represent the edge connection between nodes where N is the number of atoms, a i,j = 1 denotes a connection between node i and j, and a i,j = 0 denotes no connection. Additionally, a feature matrix X ? R N ×M is used for representing the node features of atoms where M is the dimension of feature vector that has been extracted by the algorithm aforementioned.","[0, 0, 100, 20]",5,9,113,0,0.217,0.1651376146788991,0.16363636363636364,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
The GCN layer is defined by,"[0, 0, 100, 20]",5,9,6,0,0.011,0.3333333333333333,0.3333333333333333,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,0
a ij h i = W j?N i ? {i},"[0, 0, 100, 20]",5,8.3,10,0,0.0075,0.1111111111111111,0.2222222222222222,0,0,-20,0,1,0,3.9800000000000004,0,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"denotes the node feature vector, W is the weight matrix, and N i is the neighbor node set of node i .","[0, 0, 100, 20]",5,9,22,0,0.04,0.14285714285714285,0.09523809523809523,0,0,0,0,1,1,0,1,0,0,0,1,1,1,1,0,0,0,0,1,0,0,0,1,0
"For the GAT layer, the attention coefficient of node i and j is defined as e i,j = a  Wx i , Wx j  according to 43, and is only computed when node j is in the neighborhood of node i . Then the GAT layer can be given by","[0, 0, 100, 20]",5,9,50,0,0.0835,0.1875,0.125,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"h i = a i,i Wx i + ? j?N i a i,j Wx j 3","[0, 0, 100, 20]",5,7.97,16,0,0.012,0.1875,0.1875,0,0,-20,0,1,0,0,0,0,0,0,1,1,1,1,1,0,0,0,1,0,0,0,1,0
"where x i is the node feature vector, W is the weight matrix, N i is the neighbor node set of node i, and a i,j is the normalized attention coefficients with softmax function.","[0, 0, 100, 20]",5,9,34,0,0.071,0.11428571428571428,0.05714285714285714,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"Notably, one drawback when adopting the typical GCN and GAT layers on molecular graphs is that they both dismiss the edge properties of the graph whereas the varying chemical bond types in a molecule could also impose a crucial impact on the drug?s functional mechanism. In order to tackle this problem, we encode the chemical bond type single, double, triple, and aromatic into the edge features, which can be used for updating edges in the message passing procedure in GNN models. Edge features are directly supported by GAT layer and GATv2 layer which is designed to fix the static attention problem of original GAT layer 46","[0, 0, 100, 20]",5,9,106,0,0.261,0.1509433962264151,0.08411214953271028,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"To further investigate the effectiveness of edge features, we look into RGCN and RGAT models, which consider edge types as relations and differentiate the message passing patterns according to various relation types. In molecular graphs, edges represent chemical bonds that naturally possess disparate characteristics and should be treated accordingly. Therefore, we attempt to leverage RGCN and RGAT models to represent a molecule more precisely. Considering there are R relations in total, the RGCN layer can be defined as","[0, 0, 100, 20]",5,9,78,0,0.2235,0.20512820512820512,0.1282051282051282,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
h i = W  root  x i + r?R j?N i  r,"[0, 0, 100, 20]",5,6.98,12,0,0.0095,0.16666666666666666,0.25,0,0,-20,0,1,0,2.99,0,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
Scientific Reports 2025 15:179 https://doi.org/10.1038/s41598-024-83090-3 5,"[0, 0, 100, 20]",5,8,6,0,0.035,0,0.13333333333333333,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,0
"where N i  r  denotes the neighbor node set of node i under relation r . Unlike general GCN layer, RGCN layer learns different weights specific to relation types. W  r  represents the weights corresponding to relation r, and W  root  corresponds to a special self-connected relation that is not included in R .","[0, 0, 100, 20]",6,8.99,54,0,0.1255,0.21153846153846154,0.1320754716981132,0,0,-20,0,1,0,0.02999999999999936,0,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"Similar to the way RGCN creates relation-specific transformations to update node representations, RGAT also proposes relation-specific attention weights for message aggregation. If we compute the attention coefficient of","[0, 0, 100, 20]",6,9,28,0,0.0965,0.14285714285714285,0.13333333333333333,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"h i = r?R j?N i  r i,j  r  x  j r j  5","[0, 0, 100, 20]",6,6.58,15,0,0.0105,0.11764705882352941,0.125,0,0,-20,0,1,0,2.99,0,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"where N i  r  denotes the neighbor node set of node i under relation r and a i,j  r   is the normalized attention coefficients with softmax function. Notably, the softmax function can be applied either over only attention coefficients under single relation type or all attention coefficients regardless of relation types, which result in within-relation GAT WIRGAT and across-relation GAT ARGAT, respectively. In our experiments, ARGAT is found to outperform WIRGAT and is thereby used in the subsequent analysis.","[0, 0, 100, 20]",6,9,79,0,0.215,0.15,0.10975609756097561,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"Hyperparameters such as the radius in atom feature extraction, number of neural network layers, hidden sizes and dropout rates are searched to develop the best model. We first implemented GCN and GAT-based XGDP with the features extracted with radius of 0, 1, 2 and 3, and found radius of 3 obtained the best and most stable performace on the validation set. Grid search is then conducted to find the optimal parameters for number of layers of both GNN and CNN in 1, 2, 3, 4, 5, hidden sizes in 128, 256, 512 and dropout rates in 0, 0.1, 0.2, 0.3, 0.4, 0.5 . Finally, the number of layers is set to 2 for the GNN module and 3 for the CNN module. The hidden size is set to 128. And the dropout rate is configured as 0.5.","[0, 0, 100, 20]",6,9,137,0,0.2915,0.1323529411764706,0.09090909090909091,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
Model interpretability,"[0, 0, 100, 20]",6,10,2,0,0.0105,0,0.5,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,1
"To explore our proposed model?s interpretability, we leverage on GNNExplainer 33 to identify the functional groups of molecular graphs and Integrated Gradients 34 implemented by Captum 47 to track the attributes of the genes in cancer cell line data.","[0, 0, 100, 20]",6,9,39,0,0.106,0.15384615384615385,0.125,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
Integrated gradients,"[0, 0, 100, 20]",6,9,2,0,0.0095,0.5,0.5,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,1
"Integrated Gradients is a gradient-based attribution method proposed by Subdararajan et al. 34 . Integrated Gradients is designed to satisfy two fundamental axioms, i.e., sensitivity and implementation invariance, and thus generate more reasonable explanations of neural network models than previous approaches such as Gradient * Input 48, Layer-wise Relevance propagation LRP 49, and DeepLIFT 50 .","[0, 0, 100, 20]",6,9,56,0,0.1715,0.1509433962264151,0.19642857142857142,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,0
"A prerequisite for a reasonable attribution using Integrated Gradients is to identify a baseline input. Take image networks as an example, the baseline inputs can be pixels equal to zero, constituting a black image. In our case, however, baseline cannot be simply set to zeros, since genes are seldomly expressed as zeros and picking zeros as baseline will render the explanation biased to certain genes with relatively high expression values. Our intention is to compute the average normal expression level as background for each gene, and study the effect when a gene is differentially expressed. Hypothesising that genes are normally expressed in most cell lines, we propose to identify suspiciously abnormal values with an interquartile range IQR filter. For each gene, we calculate IQR of its expression values on all the cell lines. Then expression values that are more than 2.22 times the IQR away from the median of the data are considered as outliers, which is roughly equivalent to remove the data points that have a z-score larger than 3 in the normal distribution. Thereafter we remove the outliers and compute the average of preserved expression values as the baseline of each gene.","[0, 0, 100, 20]",6,9,194,0,0.501,0.16923076923076924,0.0663265306122449,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"After deciding the baseline input, Integrated Gradients computes the integral of the gradients along the path from the baseline input to the actual input. If we denote the actual input as x and the baseline input as x', the integrated gradients can be defined by","[0, 0, 100, 20]",6,9,45,0,0.109,0.13333333333333333,0.08888888888888889,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
Attribution i  x  =  x i - x ' i   × ? a 1=0 ?F  x ' + a ×  x - x ' ?x i,"[0, 0, 100, 20]",6,8.22,26,0,0.02,0.05555555555555555,0.11764705882352941,0,0,-20,0,1,0,0,0,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,0
"where i refers to the interested dimension of inputs, and a is the interpolated value from x' to x .","[0, 0, 100, 20]",6,9,20,0,0.0405,0.05263157894736842,0,0,0,0,0,1,1,0,1,0,0,0,1,1,1,1,0,0,0,0,1,0,0,0,1,0
GNNExplainer,"[0, 0, 100, 20]",6,9,1,0,0.006,0,1,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,1
"Gradient-based methods e.g., Integrated Gradients is suitable to explain models built on grid-like data in the text or image domain. However, GNN models built in the graph domain are developed to capture the structural information of graphs, and interpreting such models requires to explore how messages are passed through the graph structures 51 . GNNExplainer is one of the explanation methods aiming at analyzing models built in the graph domain. Comparing with gradient-based methods, GNNExplainer has been testified to be capable of capturing reasonable substructures of graphs such as functional groups of molecules, in the task of molecular","[0, 0, 100, 20]",6,9,98,0,0.275,0.24742268041237114,0.07920792079207921,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
Scientific Reports 2025 15:179 https://doi.org/10.1038/s41598-024-83090-3 6,"[0, 0, 100, 20]",6,8,6,0,0.035,0,0.13333333333333333,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,0
"property prediction 33 . Therefore, in this work, we adopt GNNExplainer to interpret the graph convolutional layers and identify the active functional groups of molecular graphs.","[0, 0, 100, 20]",7,9,26,0,0.0765,0.12,0.08,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
The theory of GNNExplainer is to identify the most salient subgraph and subset of node features for the model?s prediction. It can be formulated in an optimization problem:,"[0, 0, 100, 20]",7,9,28,0,0.0725,0.14285714285714285,0.10344827586206896,1,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"max MI  Y,  G S , X S  = H  Y  - H  Y |G = G S , X = X S  7","[0, 0, 100, 20]",7,8.97,24,0,0.0145,0,0.8888888888888888,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,0
"where the mutual information MI reflects the change of model?s output when using a subgraph G S and subset of node features X S . The prediction of the model can be given by Y = F G, X , where F represents the function of the model, and G and X denote the input graph and node feature matrix. Although it is infeasible to retrieve the optimal subgraphs and feature subsets to solve the above problem directly, GNNExplainer has proposed their optimization framework to identify high-quality explanations in an empirical way.","[0, 0, 100, 20]",7,9,92,0,0.216,0.14606741573033707,0.16483516483516483,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
Drug response prediction,"[0, 0, 100, 20]",7,11,3,0,0.011,0,0.3333333333333333,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,1
"In this section, we present the results of drug sensitivity prediction and the saliency maps of inputs. We experiment with various GNN models with and without involving edge features, and compare their performance with four baseline models, i.e., tCNN 11, GraphDRP 25, DeepCDR 26 and TGSA 27 . Particularly, gene expression data are used as cell line features in place of CNV data used in the original research of tCNN and GraphDRP. DeepCDR and TGSA focused on incorporating multi-omics profiles, whereas our work intends to investigate better profiling of drug features. For a fair comparison, we used the gene expression only version of DeepCDR and TGSA to explore if our proposed method properly represents the drugs and leads to better prediction. In addition, we decode the developed models to explore the salient functional groups of drug molecules and biomarkers that are potentially responsible for the biochemical activities.","[0, 0, 100, 20]",7,9,147,0,0.394,0.13013698630136986,0.10135135135135136,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"Our models were implemented with PyTorch 52 and PyTorch Geometric 53 libraries. The performance of our experiments are evaluated by Root Mean Square Error RMSE, Pearson Correlation Coefficient PCC and Coefficient of Determination  R 2 . We performed a 3-fold cross-validation on our dataset. The mean and the standard deviation of the evaluation metrics obtained on the validation set are reported in the following sections.","[0, 0, 100, 20]",7,9,65,0,0.1795,0.140625,0.2878787878787879,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
Rediscovery of known drug and cell line responses,"[0, 0, 100, 20]",7,10,8,0,0.021,0.125,0.125,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,1
"To test XGDP with the task of rediscovering response levels of known drugs and cell lines, we randomly shuffle all the pairs of drug and cell line and divide the dataset as described above. This strategy ensures one combination of drug and cell line can present only once in training, validation or testing set, but each drug or cell line can emerge simultaneously in all sets. The rediscovery task is designed to evaluate if the model is able to learn the reaction pattern of a drug from its response data with several cell lines, and predict the response levels between the drug and other unknown cell lines.","[0, 0, 100, 20]",7,9,107,0,0.252,0.1588785046728972,0.037383177570093455,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
"Table 1 presents the performances of the proposed method with different GNN layer and compares them with the baseline models. In the table, GATE and GATv2E refer to GAT and GATv2 convolution with incorporation of bond types as edge features. It is shown that XGDP with GAT achieves the highest PCC and R 2 values, and all XGDP variants and the tCNN model achieve the lowest RMSE. DeepCDR and TGSA with only expression data obtain the worst RMSE, which is sensible since their research focus lie on incorporation of multi-omics profiles for drug response prediction. Compared with GraphDRP, our method extends the node features with the circular atomic descriptor as illustrated in Algorithm 1, and introduces multi-head attention to integrate the hidden features of drug and cell line rather than simple concatenation. It is evident in Table 1 that our refinement in the architecture leads to a better performance, especially on models with GAT convolution. Compared with tCNN, GAT- and GATE-based XGDP outperform tCNN on both PCC and R 2 . Moreover,","[0, 0, 100, 20]",7,9,172,0,0.4395,0.10526315789473684,0.1724137931034483,0,0,-20,0,1,0,0,0,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
Method Conv type RMSE  ?  PCC  ?  R 2  ? GCN 0.027 ± 0.000 0.917 ± 0.001 0.840 ± 0.003 GraphDRP 25 GAT 0.042 ± 0.002 0.828 ± 0.011 0.609 ± 0.034 GCN 0.026 ± 0.000 0.918 ± 0.001 0.843 ± 0.002 GAT 0.026 ± 0.000 0.923 ± 0.000 0.851 ± 0.001 GATE 0.026 ± 0.000 0.922 ± 0.001 0.849 ± 0.001 XGDP GATv2E 0.026 ± 0.000 0.921 ± 0.001 0.846 ± 0.001 RGCN 0.026 ± 0.000 0.920 ± 0.001 0.845 ± 0.001 RGAT 0.026 ± 0.000 0.920 ± 0.001 0.846 ± 0.002,"[0, 0, 100, 20]",7,7,93,0,0.167,0.08888888888888889,0.13157894736842105,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,1,0,0,0,0,1,0,0,0,1,0
"Table 1 . Performance of proposed and baseline models in the task of rediscovering known drug and cell line responses. All the models, except GraghDRP-GAT, achieve similar RMSE ~0.26. Best PCC and R 2 marked in bold is achieved by XGDP-GAT.","[0, 0, 100, 20]",7,9,41,0,0.1,0.175,0.2558139534883721,0,0,-20,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0
Scientific Reports 2025 15:179 https://doi.org/10.1038/s41598-024-83090-3 7,"[0, 0, 100, 20]",7,8,6,0,0.035,0,0.13333333333333333,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,1,0
